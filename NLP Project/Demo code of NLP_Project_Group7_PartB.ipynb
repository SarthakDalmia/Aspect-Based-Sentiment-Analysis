{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demo code of NLP_Project_Group7_PartB.ipynb","provenance":[{"file_id":"1tSgF1WaRVI0WkKdzw9bw3mrLnsm7F3sD","timestamp":1638779626586}],"collapsed_sections":["Od4uB_ORsqQF","77Xb5KS5FFEp","oFBWz70ewcwU"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Od4uB_ORsqQF"},"source":["# Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nx7hlvFp0vGL","executionInfo":{"status":"ok","timestamp":1638778440233,"user_tz":-330,"elapsed":56080,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"684e5cbc-7f67-44b3-f701-a6777e467a81"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7PcgfvLSCPT","executionInfo":{"status":"ok","timestamp":1638778625784,"user_tz":-330,"elapsed":185586,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"9d2c793c-7946-4595-d20e-139f93dc21c1"},"source":["!pip install torch==1.4.0\n","import torch\n","print(torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.4.0\n","  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[K     |████████████████████████████████| 753.4 MB 6.7 kB/s \n","\u001b[?25hInstalling collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.10.0+cu111\n","    Uninstalling torch-1.10.0+cu111:\n","      Successfully uninstalled torch-1.10.0+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.4.0\n","1.4.0\n"]}]},{"cell_type":"code","metadata":{"id":"MRYTkvaA8Os3"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYoRocQhGeAp","executionInfo":{"status":"ok","timestamp":1638778691695,"user_tz":-330,"elapsed":65937,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"df5f0563-58a3-40f2-a86f-d498992075b9"},"source":["!pip install stanfordnlp\n","import stanfordnlp\n","stanfordnlp.download('en')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting stanfordnlp\n","  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n","\u001b[?25l\r\u001b[K     |██                              | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 158 kB 16.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (4.62.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.4.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (2.23.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanfordnlp) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.10)\n","Installing collected packages: stanfordnlp\n","Successfully installed stanfordnlp-0.2.0\n","Using the default treebank \"en_ewt\" for language \"en\".\n","Would you like to download the models for: en_ewt now? (Y/n)\n","Y\n","\n","Default download directory: /root/stanfordnlp_resources\n","Hit enter to continue or type an alternate directory.\n","\n","\n","Downloading models for: en_ewt\n","Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 235M/235M [00:41<00:00, 5.64MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n","Extracting models file for: en_ewt\n","Cleaning up...Done.\n"]}]},{"cell_type":"code","metadata":{"id":"04-sKY__1t58"},"source":["from xml.etree import cElementTree as ET\n","import pandas as pd\n","import string\n","import spacy\n","import re\n","import random\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","import csv\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gsszVN_1NAJ"},"source":["root_dir = \"/content/drive/MyDrive/Acads/4-1/NLP/NLP Project/Topic 5/\"\n","train_folder = \"Train/SemEval'14-ABSA-TrainData_v2 & AnnotationGuidelines/\"\n","laptop_train_file = \"Laptop_Train_v2.xml\"\n","restaurants_train_file = \"Restaurants_Train_v2.xml\"\n","test_1_folder = \"Test1/ABSA_TestData_PhaseA/ABSA_TestData_PhaseA/\"\n","laptop_test_1_file = \"Laptops_Test_Data_PhaseA.xml\"\n","restaurants_test_1_file = \"Restaurants_Test_Data_PhaseA.xml\"\n","test_2_folder = \"Test2/ABSA_TestData_PhaseB/\"\n","laptop_test_2_file = \"Laptops_Test_Data_phaseB.xml\"\n","restaurants_test_2_file = \"Restaurants_Test_Data_phaseB.xml\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Y6IZZl-QpY3"},"source":["# Data Extraction & Cleaning"]},{"cell_type":"code","metadata":{"id":"O8tKAkEGTDgE"},"source":["class AspectWord():\n","    def __init__(self, aspect_term, sentiment_terms, polarity = None):\n","        # Words\n","        self.aspect_term = aspect_term\n","        # List of Words\n","        self.sentiment_terms = sentiment_terms\n","        # Polarity: +,-,neutral\n","        self.polarity = polarity\n","\n","    def __str__(self):\n","        return f\"Aspect Term: {self.aspect_term} Sentiment Terms: {self.sentiment_terms} Polarity: {self.polarity}\"\n","    \n","    def __repr__(self):\n","        return self.__str__()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEoYTdWZ9Gpr"},"source":["class Sentence():\n","    def __init__(self, sentence_id, sentence, data_type = \"Train\",\n","                 actual_sentence_id = None, actual_aspect_words = []): \n","        # Int\n","        self.sentence_id = sentence_id\n","        # String\n","        self.sentence = sentence\n","        # String\n","        self.data_type = data_type\n","        # String\n","        self.actual_sentence_id = actual_sentence_id\n","        # List of AspectWord\n","        self.actual_aspect_words = actual_aspect_words\n","        # List of AspectWord\n","        self.generated_aspect_words = []\n","\n","    def __str__(self):\n","        if self.data_type == \"Train\":\n","            return f\"ID: {self.sentence_id} Sentence: {self.sentence} {self.actual_aspect_words}\"\n","        else:\n","            return f\"ID: {self.sentence_id} Sentence: {self.sentence} {self.generated_aspect_words}\"\n","    \n","    def __repr__(self):\n","        return self.__str__()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIb2pocS9AiG"},"source":["def xml_to_sentences(path,data_type = \"Train\"):\n","    data = []\n","    tree = ET.parse(path)\n","    root = tree.getroot()\n","    id = 1\n","    for page in root.findall('sentence'):\n","        sentence_id = id\n","        sentence = page[0].text\n","        actual_sentence_id = page.attrib[\"id\"]\n","        temp = []\n","        for i in range(1,len(page)):\n","            temp.append([x.attrib for x in page[i]])\n","        actual_aspect_terms = []\n","        actual_polarity = []\n","        if (data_type == \"Train\"):\n","            if len(temp) > 0:\n","            # Ignore Sentences without aspect terms if training data\n","                aspect_words = []\n","                for x in temp[0]:\n","                    aspect_words.append(AspectWord(x['term'],[],x['polarity']))\n","                id += 1\n","                data.append(Sentence(sentence_id, sentence, data_type, actual_sentence_id, aspect_words))\n","        elif (data_type == \"Test\"):\n","            data.append(Sentence(sentence_id, sentence, data_type, actual_sentence_id))\n","            id += 1\n","        else:\n","            print(\"Incorrect Data Type\")\n","            return None\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3gJ47H_EAw-"},"source":["def data_list_to_df(data_list, aspect_terms = \"actual\", aspect_polarity = True):\n","    \n","    data = pd.DataFrame(columns = [\"ID\",\"Sentence\",\"Actual Aspect Terms\",\"Actual Sentiment Terms\",\"Actual Polarities\",\"Generated Aspect Terms\",\"Generated Sentiment Terms\",\"Generated Polarities\"])\n","    \n","    for i in range(len(data_list)):\n","        data.loc[len(data.index)] = [data_list[i].sentence_id, data_list[i].sentence, [x.aspect_term for x in data_list[i].actual_aspect_words], [x.sentiment_terms for x in data_list[i].actual_aspect_words], [x.polarity for x in data_list[i].actual_aspect_words], [x.aspect_term for x in data_list[i].generated_aspect_words], [x.sentiment_terms for x in data_list[i].generated_aspect_words], [x.polarity for x in data_list[i].generated_aspect_words]] \n","    \n","    if aspect_terms == \"actual\":\n","        data.drop([\"Generated Aspect Terms\",\"Generated Sentiment Terms\",\"Generated Polarities\",\"Actual Sentiment Terms\"], axis = 1, inplace = True)\n","    elif aspect_terms == \"generated\":\n","        data.drop([\"Actual Aspect Terms\",\"Actual Sentiment Terms\",\"Actual Polarities\"], axis = 1, inplace = True)\n","    elif aspect_terms == \"none\":\n","        data.drop([\"Generated Aspect Terms\",\"Generated Sentiment Terms\",\"Generated Polarities\",\"Actual Aspect Terms\",\"Actual Sentiment Terms\",\"Actual Polarities\"], axis = 1, inplace = True)\n","\n","    if not aspect_polarity:\n","        try:\n","            data.drop([\"Generated Polarities\"],axis=1, inplace = True)\n","        except:\n","            pass\n","        try:\n","            data.drop([\"Actual Polarities\"],axis=1, inplace = True)\n","        except:\n","            pass\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"R_bW8EQv1o_d","executionInfo":{"status":"ok","timestamp":1638778701849,"user_tz":-330,"elapsed":9310,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"1fc0b550-7253-4688-d0c8-9e4e3c4c9bd3"},"source":["laptop_train_data = xml_to_sentences(root_dir + train_folder + laptop_train_file)\n","laptop_train_data_size = len(laptop_train_data)\n","print(f\"Total sentences: {laptop_train_data_size}\")\n","laptop_train_data_df = data_list_to_df(laptop_train_data, aspect_terms = \"actual\", aspect_polarity = True)\n","laptop_train_data_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sentences: 1488\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sentence</th>\n","      <th>Actual Aspect Terms</th>\n","      <th>Actual Polarities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>I charge it at night and skip taking the cord ...</td>\n","      <td>[cord, battery life]</td>\n","      <td>[neutral, positive]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>The tech guy then said the service center does...</td>\n","      <td>[service center, \"sales\" team, tech guy]</td>\n","      <td>[negative, negative, neutral]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>it is of high quality, has a killer GUI, is ex...</td>\n","      <td>[quality, GUI, applications, use]</td>\n","      <td>[positive, positive, positive, positive]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Easy to start up and does not overheat as much...</td>\n","      <td>[start up]</td>\n","      <td>[positive]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>I even got my teenage son one, because of the ...</td>\n","      <td>[features, iChat, Photobooth, garage band]</td>\n","      <td>[positive, positive, positive, positive]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  ID  ...                         Actual Polarities\n","0  1  ...                       [neutral, positive]\n","1  2  ...             [negative, negative, neutral]\n","2  3  ...  [positive, positive, positive, positive]\n","3  4  ...                                [positive]\n","4  5  ...  [positive, positive, positive, positive]\n","\n","[5 rows x 4 columns]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"OvksVdL85A9I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638778701850,"user_tz":-330,"elapsed":18,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"bc8c0dd7-d609-4f22-f745-485f37074f85"},"source":["# Split into train and validation\n","np.random.seed(7)\n","random.seed(9)\n","laptop_train_data, laptop_valid_data = sklearn.model_selection.train_test_split(laptop_train_data, test_size=0.1)\n","print(f\"Size of Training Data:{len(laptop_valid_data)}, Size of Validation Data: {len(laptop_train_data)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of Training Data:149, Size of Validation Data: 1339\n"]}]},{"cell_type":"markdown","metadata":{"id":"77Xb5KS5FFEp"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"yzboBF5Ls_CL"},"source":["# Remove Punctuations - {, . \"\" ' ;}\n","# Check stopword Removal"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z9yUW3JYsyg0"},"source":["# Find Aspect Terms"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mv7_H0EAQxOC","executionInfo":{"status":"ok","timestamp":1638778704263,"user_tz":-330,"elapsed":2422,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"db06a45d-0cab-4fa9-91ca-575cbc1d2cc7"},"source":["nlp = stanfordnlp.Pipeline()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use device: cpu\n","---\n","Loading: tokenize\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n","---\n","Loading: pos\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n","---\n","Loading: lemma\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n","Building an attentional Seq2Seq model...\n","Using a Bi-LSTM encoder\n","Using soft attention for LSTM.\n","Finetune all embeddings.\n","[Running seq2seq lemmatizer with edit classifier]\n","---\n","Loading: depparse\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n","Done loading processors!\n","---\n"]}]},{"cell_type":"code","metadata":{"id":"aaNQFY8cqUm1"},"source":["def extract_attributes(text):\n","    doc = nlp(text)\n","    parsed_text = {'word':[], 'lemma':[], 'pos':[], 'dep':[], 'gov':[], 'gov_pos':[]}\n","    for sent in doc.sentences:\n","        for wrd in sent.words:\n","            parsed_text['word'].append(wrd.text)\n","            parsed_text['lemma'].append(wrd.lemma)\n","            parsed_text['pos'].append(wrd.pos)\n","            parsed_text['dep'].append(wrd.dependency_relation)\n","    for sent in doc.sentences:\n","        for wrd in sent.words:        \n","            if wrd.governor == 0:\n","                parsed_text['gov'].append(\"ROOT\")\n","                parsed_text['gov_pos'].append(\"-\")\n","            else:   \n","                parsed_text['gov'].append(parsed_text['word'][wrd.governor-1])\n","                parsed_text['gov_pos'].append(parsed_text['pos'][wrd.governor-1])\n","    \n","    return pd.DataFrame(parsed_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":895},"id":"bu7SkTslyip8","executionInfo":{"status":"ok","timestamp":1638778705080,"user_tz":-330,"elapsed":822,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"e30292f1-cea8-4294-c887-8e061a05f59e"},"source":["text = \"The display on this computer is the best I've seen in a very long time, the battery life is very long and very convenient.\"\n","extract_attributes(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>lemma</th>\n","      <th>pos</th>\n","      <th>dep</th>\n","      <th>gov</th>\n","      <th>gov_pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The</td>\n","      <td>the</td>\n","      <td>DT</td>\n","      <td>det</td>\n","      <td>display</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>display</td>\n","      <td>display</td>\n","      <td>NN</td>\n","      <td>nsubj</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>on</td>\n","      <td>on</td>\n","      <td>IN</td>\n","      <td>case</td>\n","      <td>computer</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>this</td>\n","      <td>this</td>\n","      <td>DT</td>\n","      <td>det</td>\n","      <td>computer</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>computer</td>\n","      <td>computer</td>\n","      <td>NN</td>\n","      <td>nmod</td>\n","      <td>display</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>is</td>\n","      <td>be</td>\n","      <td>VBZ</td>\n","      <td>cop</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>the</td>\n","      <td>the</td>\n","      <td>DT</td>\n","      <td>det</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>best</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","      <td>root</td>\n","      <td>ROOT</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>I</td>\n","      <td>I</td>\n","      <td>PRP</td>\n","      <td>nsubj</td>\n","      <td>seen</td>\n","      <td>VBN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>'ve</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>aux</td>\n","      <td>seen</td>\n","      <td>VBN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>seen</td>\n","      <td>see</td>\n","      <td>VBN</td>\n","      <td>acl:relcl</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>in</td>\n","      <td>in</td>\n","      <td>IN</td>\n","      <td>case</td>\n","      <td>time</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>DT</td>\n","      <td>det</td>\n","      <td>time</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>very</td>\n","      <td>very</td>\n","      <td>RB</td>\n","      <td>advmod</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>long</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","      <td>amod</td>\n","      <td>time</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>time</td>\n","      <td>time</td>\n","      <td>NN</td>\n","      <td>obl</td>\n","      <td>seen</td>\n","      <td>VBN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>punct</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>the</td>\n","      <td>the</td>\n","      <td>DT</td>\n","      <td>det</td>\n","      <td>life</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>battery</td>\n","      <td>battery</td>\n","      <td>NN</td>\n","      <td>compound</td>\n","      <td>life</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>life</td>\n","      <td>life</td>\n","      <td>NN</td>\n","      <td>nsubj</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>is</td>\n","      <td>be</td>\n","      <td>VBZ</td>\n","      <td>cop</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>very</td>\n","      <td>very</td>\n","      <td>RB</td>\n","      <td>advmod</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>long</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","      <td>parataxis</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>and</td>\n","      <td>and</td>\n","      <td>CC</td>\n","      <td>cc</td>\n","      <td>convenient</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>very</td>\n","      <td>very</td>\n","      <td>RB</td>\n","      <td>advmod</td>\n","      <td>convenient</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>convenient</td>\n","      <td>convenient</td>\n","      <td>JJ</td>\n","      <td>conj</td>\n","      <td>long</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>punct</td>\n","      <td>best</td>\n","      <td>JJS</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          word       lemma  pos        dep         gov gov_pos\n","0          The         the   DT        det     display      NN\n","1      display     display   NN      nsubj        best     JJS\n","2           on          on   IN       case    computer      NN\n","3         this        this   DT        det    computer      NN\n","4     computer    computer   NN       nmod     display      NN\n","5           is          be  VBZ        cop        best     JJS\n","6          the         the   DT        det        best     JJS\n","7         best        best  JJS       root        ROOT       -\n","8            I           I  PRP      nsubj        seen     VBN\n","9          've        have  VBP        aux        seen     VBN\n","10        seen         see  VBN  acl:relcl        best     JJS\n","11          in          in   IN       case        time      NN\n","12           a           a   DT        det        time      NN\n","13        very        very   RB     advmod        long      JJ\n","14        long        long   JJ       amod        time      NN\n","15        time        time   NN        obl        seen     VBN\n","16           ,           ,    ,      punct        long      JJ\n","17         the         the   DT        det        life      NN\n","18     battery     battery   NN   compound        life      NN\n","19        life        life   NN      nsubj        long      JJ\n","20          is          be  VBZ        cop        long      JJ\n","21        very        very   RB     advmod        long      JJ\n","22        long        long   JJ  parataxis        best     JJS\n","23         and         and   CC         cc  convenient      JJ\n","24        very        very   RB     advmod  convenient      JJ\n","25  convenient  convenient   JJ       conj        long      JJ\n","26           .           .    .      punct        best     JJS"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"id":"vlOCkxGza4IX","executionInfo":{"status":"ok","timestamp":1638778705084,"user_tz":-330,"elapsed":35,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"d0e1b2cc-f210-4b07-d780-cb413a36b2fe"},"source":["text = \"Bell, based in Los Angeles, makes and distributes electronic, computer and building products.\"\n","extract_attributes(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>lemma</th>\n","      <th>pos</th>\n","      <th>dep</th>\n","      <th>gov</th>\n","      <th>gov_pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Bell</td>\n","      <td>Bell</td>\n","      <td>NNP</td>\n","      <td>nsubj</td>\n","      <td>makes</td>\n","      <td>VBZ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>punct</td>\n","      <td>Bell</td>\n","      <td>NNP</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>based</td>\n","      <td>base</td>\n","      <td>VBN</td>\n","      <td>case</td>\n","      <td>Angeles</td>\n","      <td>NNP</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in</td>\n","      <td>in</td>\n","      <td>IN</td>\n","      <td>case</td>\n","      <td>Angeles</td>\n","      <td>NNP</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Los</td>\n","      <td>Los</td>\n","      <td>NNP</td>\n","      <td>compound</td>\n","      <td>Angeles</td>\n","      <td>NNP</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Angeles</td>\n","      <td>Angeles</td>\n","      <td>NNP</td>\n","      <td>nmod</td>\n","      <td>Bell</td>\n","      <td>NNP</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>punct</td>\n","      <td>makes</td>\n","      <td>VBZ</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>makes</td>\n","      <td>make</td>\n","      <td>VBZ</td>\n","      <td>root</td>\n","      <td>ROOT</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>and</td>\n","      <td>and</td>\n","      <td>CC</td>\n","      <td>cc</td>\n","      <td>distributes</td>\n","      <td>VBZ</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>distributes</td>\n","      <td>distribute</td>\n","      <td>VBZ</td>\n","      <td>conj</td>\n","      <td>makes</td>\n","      <td>VBZ</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>electronic</td>\n","      <td>electronic</td>\n","      <td>JJ</td>\n","      <td>amod</td>\n","      <td>products</td>\n","      <td>NNS</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>punct</td>\n","      <td>computer</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>computer</td>\n","      <td>computer</td>\n","      <td>NN</td>\n","      <td>conj</td>\n","      <td>electronic</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>and</td>\n","      <td>and</td>\n","      <td>CC</td>\n","      <td>cc</td>\n","      <td>building</td>\n","      <td>NN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>building</td>\n","      <td>building</td>\n","      <td>NN</td>\n","      <td>conj</td>\n","      <td>electronic</td>\n","      <td>JJ</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>products</td>\n","      <td>product</td>\n","      <td>NNS</td>\n","      <td>obj</td>\n","      <td>distributes</td>\n","      <td>VBZ</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>punct</td>\n","      <td>makes</td>\n","      <td>VBZ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           word       lemma  pos       dep          gov gov_pos\n","0          Bell        Bell  NNP     nsubj        makes     VBZ\n","1             ,           ,    ,     punct         Bell     NNP\n","2         based        base  VBN      case      Angeles     NNP\n","3            in          in   IN      case      Angeles     NNP\n","4           Los         Los  NNP  compound      Angeles     NNP\n","5       Angeles     Angeles  NNP      nmod         Bell     NNP\n","6             ,           ,    ,     punct        makes     VBZ\n","7         makes        make  VBZ      root         ROOT       -\n","8           and         and   CC        cc  distributes     VBZ\n","9   distributes  distribute  VBZ      conj        makes     VBZ\n","10   electronic  electronic   JJ      amod     products     NNS\n","11            ,           ,    ,     punct     computer      NN\n","12     computer    computer   NN      conj   electronic      JJ\n","13          and         and   CC        cc     building      NN\n","14     building    building   NN      conj   electronic      JJ\n","15     products     product  NNS       obj  distributes     VBZ\n","16            .           .    .     punct        makes     VBZ"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"0kC4ChZC9SMm"},"source":["def parse_compound_aspects(data):\n","    count = 0\n","    for i in range(len(data)):\n","        j = 0\n","        generated_aspect_words_to_be_deleted = set()\n","        while(j < len(data[i].generated_aspect_words)):\n","            generated_aspect_term = data[i].generated_aspect_words[j].aspect_term\n","            individual_aspect_terms = generated_aspect_term.split(\" \")\n","            if(len(individual_aspect_terms) > 1):\n","                for individual_aspect_term in individual_aspect_terms:\n","                    # print(f\"Generated: {data[i].generated_aspect_words}\")\n","                    k = 0\n","                    while(k < len(data[i].generated_aspect_words)):\n","                        if k == j:\n","                            k += 1\n","                            continue\n","                        # print(f\"i : {i} j : {j} k : {k} {individual_aspect_term}\")\n","                        if individual_aspect_term == data[i].generated_aspect_words[k].aspect_term:\n","                            data[i].generated_aspect_words[j].sentiment_terms += (data[i].generated_aspect_words[k].sentiment_terms)\n","                            generated_aspect_words_to_be_deleted.add(data[i].generated_aspect_words[k])\n","                            # print(f\"Compound: {generated_aspect_term}, Deleting {data[i].generated_aspect_words[k]}\")\n","                            count += 1\n","                        k += 1\n","            j += 1\n","        for generated_aspect_word_to_be_deleted in generated_aspect_words_to_be_deleted:\n","            data[i].generated_aspect_words.remove(generated_aspect_word_to_be_deleted)\n","    return count\n","\n","def add_or_update_aspect_word(sentence, aspect_term, sentiment_term):\n","    if sentiment_term == None:\n","        sentence.generated_aspect_words.append(AspectWord(aspect_term, []))\n","        return True\n","    for i in range(len(sentence.generated_aspect_words)):\n","        if sentence.generated_aspect_words[i].aspect_term == aspect_term:\n","            sentence.generated_aspect_words[i].sentiment_terms.append(sentiment_term)\n","            return True\n","\n","    sentence.generated_aspect_words.append(AspectWord(aspect_term, [sentiment_term]))\n","    return True\n","\n","def generate_aspect_words(data):\n","    dependencies= {\"nsubj\":[],\"amod\":[],\"dobj\":[],\"nmod\":[],\"acl\":[],\"conj\":[],\"compound\":[]}\n","        \n","    for i in range(len(data)):\n","        extracted_attributes_df = extract_attributes(data[i].sentence)\n","        data[i].generated_aspect_words.clear()\n","\n","        # Rules:\n","        for j in extracted_attributes_df.index:\n","            \n","            # Bell, based in Los Angeles, makes and distributes electronic, computer and building products.\n","            if extracted_attributes_df['dep'][j] == \"nsubj\":\n","                # Example: (best, display)\n","                if extracted_attributes_df['gov_pos'][j] in [\"JJ\", \"JJR\", \"JJS\"] and extracted_attributes_df['pos'][j] in [\"NN\", \"NNS\", \"NNP\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j], extracted_attributes_df['gov'][j])\n","                # Example: (distributes, XYZ Company)\n","                elif extracted_attributes_df['gov_pos'][j] in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"] and extracted_attributes_df['pos'][j] in [\"NN\", \"NNS\", \"NNP\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j], extracted_attributes_df['gov'][j])\n","                # dependencies[\"nsubj\"].append(((extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j]),(extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j])))\n","            \n","            elif extracted_attributes_df['dep'][j] == \"amod\":\n","                # Example: (products, electronic)\n","                if extracted_attributes_df['gov_pos'][j] in [\"NN\", \"NNS\", \"NNP\"] and extracted_attributes_df['pos'][j] in [\"JJ\", \"JJR\", \"JJS\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['gov'][j], extracted_attributes_df['word'][j])\n","                # Example: (products, building)\n","                elif extracted_attributes_df['gov_pos'][j] in [\"NN\", \"NNS\", \"NNP\"] and extracted_attributes_df['pos'][j] in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['gov'][j], extracted_attributes_df['word'][j])\n","                # dependencies[\"amod\"].append(((extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j]),(extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j])))\n","            \n","            elif extracted_attributes_df['dep'][j] == \"obj\":\n","                # Example: (makes, products)\n","                if extracted_attributes_df['gov_pos'][j] in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"] and extracted_attributes_df['pos'][j] in [\"NN\", \"NNS\", \"NNP\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j], extracted_attributes_df['gov'][j])\n","                    dependencies[\"dobj\"].append(((extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j]),(extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j])))\n","\n","            elif extracted_attributes_df['dep'][j] == \"nmod\":\n","                # Both are aspects\n","                # Example: (display, computer)\n","                if extracted_attributes_df['gov_pos'][j] in [\"NN\"] and extracted_attributes_df['pos'][j] in [\"NN\", \"NS\"]:\n","                    # Check which before\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j] + \" \" + extracted_attributes_df['gov'][j], None)\n","                    # dependencies[\"nmod\"].append(((extracted_attributes_df['word'][j] + \" \" + extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j],extracted_attributes_df['pos'][j])))\n","\n","                elif extracted_attributes_df['gov_pos'][j] in [\"JJ\"] and extracted_attributes_df['pos'][j] in [\"NN\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j], extracted_attributes_df['gov'][j])\n","                    # dependencies[\"nmod\"].append(((extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j]),(extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j])))\n","\n","            elif extracted_attributes_df['dep'][j] == \"acl\":\n","                if extracted_attributes_df['gov_pos'][j] in [\"NN\"] and extracted_attributes_df['pos'][j] in [\"JJ\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['gov'][j], extracted_attributes_df['word'][j])\n","                \n","                elif extracted_attributes_df['gov_pos'][j] in [\"NNS\"] and extracted_attributes_df['pos'][j] in [\"VBP\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['gov'][j], extracted_attributes_df['word'][j])\n","                # dependencies[\"acl\"].append(((extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j]),(extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j])))\n","\n","            elif extracted_attributes_df['dep'][j] == \"conj\":\n","                # Both are aspects\n","                if extracted_attributes_df['gov_pos'][j] in [\"NN\"] and extracted_attributes_df['pos'][j] in [\"NN\", \"NNS\", \"NNP\"]:\n","                    # Check which before\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j] + \" \" + extracted_attributes_df['gov'][j], None)\n","                    # dependencies[\"conj\"].append(((extracted_attributes_df['word'][j] + \" \" + extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j],extracted_attributes_df['pos'][j])))\n","\n","                elif extracted_attributes_df['gov_pos'][j] in [\"NN\", \"NNS\", \"NNP\"] and extracted_attributes_df['pos'][j] in [\"JJ\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['gov'][j], extracted_attributes_df['word'][j])\n","                    # dependencies[\"conj\"].append(((extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j]),(extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j])))\n","\n","                elif extracted_attributes_df['gov_pos'][j] in [\"NN\", \"NNS\", \"NNP\"] and extracted_attributes_df['pos'][j] in [\"VBZ\"]:\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['gov'][j], extracted_attributes_df['word'][j])\n","                    # dependencies[\"conj\"].append(((extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j]),(extracted_attributes_df['word'][j],extracted_attributes_df['pos'][j])))\n","\n","            elif extracted_attributes_df['dep'][j] == \"compound\":\n","                # Both are aspects\n","                if extracted_attributes_df['gov_pos'][j] in [\"NN\"] and extracted_attributes_df['pos'][j] in [\"NN\"]:\n","                    # Check which before\n","                    add_or_update_aspect_word(data[i], extracted_attributes_df['word'][j] + \" \" + extracted_attributes_df['gov'][j], None)\n","                    # dependencies[\"compound\"].append(((extracted_attributes_df['word'][j] + \" \" + extracted_attributes_df['gov'][j],extracted_attributes_df['gov_pos'][j],extracted_attributes_df['pos'][j])))\n","    parse_compound_aspects(data)\n","    return dependencies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"B54ACZ0ADxbJ","executionInfo":{"status":"ok","timestamp":1638778920788,"user_tz":-330,"elapsed":215733,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"091bfc08-7a9e-45fe-e2cd-3ca401d0499d"},"source":["train_dependencies = generate_aspect_words(laptop_train_data)\n","laptop_train_data_df = data_list_to_df(laptop_train_data, aspect_terms = \"generated\", aspect_polarity = True)\n","laptop_train_data_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sentence</th>\n","      <th>Generated Aspect Terms</th>\n","      <th>Generated Sentiment Terms</th>\n","      <th>Generated Polarities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>611</td>\n","      <td>Not only are the versions of these programs ab...</td>\n","      <td>[versions]</td>\n","      <td>[[worked, able, superior]]</td>\n","      <td>[None]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>260</td>\n","      <td>Sometimes you really have to tap the pad to ge...</td>\n","      <td>[pad]</td>\n","      <td>[[tap]]</td>\n","      <td>[None]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>799</td>\n","      <td>-When battery life went to 4 hours or less, to...</td>\n","      <td>[battery life]</td>\n","      <td>[[went]]</td>\n","      <td>[None]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>397</td>\n","      <td>If you don't feel comfortable doing it yoursel...</td>\n","      <td>[case, one, Buy]</td>\n","      <td>[[buy], [white, bought], [Best]]</td>\n","      <td>[None, None, None]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>834</td>\n","      <td>/ awesome cooling system/ much better grafics ...</td>\n","      <td>[cooling system, card, GB RAM, RAM screen, bac...</td>\n","      <td>[[awesome], [better], [], [LED], [LED], [LED, ...</td>\n","      <td>[None, None, None, None, None, None]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    ID  ...                  Generated Polarities\n","0  611  ...                                [None]\n","1  260  ...                                [None]\n","2  799  ...                                [None]\n","3  397  ...                    [None, None, None]\n","4  834  ...  [None, None, None, None, None, None]\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"v60PCbv3R5-7"},"source":["# import sys\n","\n","# orig_stdout = sys.stdout\n","# f = open(root_dir + train_folder + \"train_dependencies.txt\", 'w')\n","# sys.stdout = f\n","\n","# for k,v in train_dependencies.items():\n","#     print(f\"{k}\\n\")\n","#     for x in v:\n","#         print(f\"\\t{x}\")\n","\n","# sys.stdout = orig_stdout\n","# f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQnzE7xzeot_"},"source":["def match_aspect_terms(actual_aspect_term, generated_aspect_term):\n","    n = len(generated_aspect_term.split(\" \"))\n","    for i in range(n):\n","        temp = generated_aspect_term.split(\" \",maxsplit = i)\n","        if len(temp) > 1:\n","            rest = temp[1]\n","        else:\n","            rest = temp[0]\n","        if re.search(rest, actual_aspect_term):\n","            return len(rest)/len(actual_aspect_term)\n","        generated_aspect_term = rest\n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fR9eSBUA03s"},"source":["def aspect_term_performance(data):\n","    true_positives = 0\n","    false_positives = 0\n","    no_of_actual_aspect_terms = 0\n","    no_of_generated_aspect_terms = 0\n","\n","    for i in range(len(data)):\n","        no_of_generated_aspect_terms += len(data[i].generated_aspect_words)\n","        no_of_actual_aspect_terms += len(data[i].actual_aspect_words)\n","        for generated_aspect_word in data[i].generated_aspect_words:\n","            found = False\n","            for actual_aspect_word in data[i].actual_aspect_words:\n","                matched_part = match_aspect_terms(actual_aspect_word.aspect_term, generated_aspect_word.aspect_term)\n","                if matched_part > 0:\n","                    true_positives += matched_part\n","                    found = True\n","                    break\n","            if not found:\n","                false_positives += 1\n","\n","    ratios = {\"Aspect Term Extraction Precision\": true_positives / no_of_generated_aspect_terms, \"Aspect Term Extraction Recall\":true_positives/no_of_actual_aspect_terms, \"Aspect Term Extraction True Positives\":true_positives, \"Actual Aspect Terms\":no_of_actual_aspect_terms, \"Generated Aspect Terms\":no_of_generated_aspect_terms}\n","    return ratios"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"w49l-LwWGc23","executionInfo":{"status":"ok","timestamp":1638778922512,"user_tz":-330,"elapsed":27,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"dd362def-bbe1-4922-e7ba-338783bbc5db"},"source":["laptop_train_data_ratios = aspect_term_performance(laptop_train_data)\n","pd.DataFrame.from_dict(laptop_train_data_ratios, orient = 'index',columns = [\"Parameters\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Parameters</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Aspect Term Extraction Precision</th>\n","      <td>0.419667</td>\n","    </tr>\n","    <tr>\n","      <th>Aspect Term Extraction Recall</th>\n","      <td>0.566810</td>\n","    </tr>\n","    <tr>\n","      <th>Aspect Term Extraction True Positives</th>\n","      <td>1191.433998</td>\n","    </tr>\n","    <tr>\n","      <th>Actual Aspect Terms</th>\n","      <td>2102.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Generated Aspect Terms</th>\n","      <td>2839.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        Parameters\n","Aspect Term Extraction Precision          0.419667\n","Aspect Term Extraction Recall             0.566810\n","Aspect Term Extraction True Positives  1191.433998\n","Actual Aspect Terms                    2102.000000\n","Generated Aspect Terms                 2839.000000"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"HXny_pPTtgsh"},"source":["# Train the Model\n","Make a frequency dictionary for all the Sentiment Words"]},{"cell_type":"code","metadata":{"id":"SFMs7p3YtU6j"},"source":["def train(data):\n","    # Do preprocessing if needed\n","    sentiment_terms_frequency = {}\n","    for sentence in data:\n","        for generated_aspect_word in sentence.generated_aspect_words:\n","            polarity = None\n","            for actual_aspect_word in sentence.actual_aspect_words:\n","                matched_part = match_aspect_terms(actual_aspect_word.aspect_term, generated_aspect_word.aspect_term)\n","                if matched_part > 0: \n","                    if actual_aspect_word.polarity == 'positive':\n","                        polarity = 1\n","                    \n","                    elif actual_aspect_word.polarity == 'negative':\n","                        polarity = -1\n","                    \n","                    else:\n","                        polarity = 0\n","                    break    \n","            if polarity ==  None:\n","                continue\n","            for sentiment_term in generated_aspect_word.sentiment_terms:\n","                if sentiment_term in list(sentiment_terms_frequency.keys()): \n","                    sentiment_terms_frequency[sentiment_term]['frequency'] += 1\n","                    sentiment_terms_frequency[sentiment_term]['polarity'] += (polarity)\n","                \n","                else:\n","                    sentiment_terms_frequency[sentiment_term] = {'frequency':1, 'polarity':polarity}\n","\n","    for key in list(sentiment_terms_frequency.keys()):\n","        sentiment_terms_frequency[key] = (sentiment_terms_frequency[key]['polarity']/sentiment_terms_frequency[key]['frequency'])\n","\n","    return sentiment_terms_frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"f9CeSkQu9v9P","executionInfo":{"status":"ok","timestamp":1638779072020,"user_tz":-330,"elapsed":648,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"f75f7c3a-2b5c-40fd-ef9b-88cc50fafc8c"},"source":["laptop_sentiment_terms_frequency_train = train(laptop_train_data)\n","print(f\"Number of Sentiment Terms: {len(laptop_sentiment_terms_frequency_train)}\")\n","df = pd.DataFrame.from_dict(laptop_sentiment_terms_frequency_train, orient = 'index', columns =[\"Polarity\"])\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Sentiment Terms: 615\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>tap</th>\n","      <td>-1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>went</th>\n","      <td>-0.833333</td>\n","    </tr>\n","    <tr>\n","      <th>buy</th>\n","      <td>-0.333333</td>\n","    </tr>\n","    <tr>\n","      <th>awesome</th>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>better</th>\n","      <td>0.142857</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Polarity\n","tap     -1.000000\n","went    -0.833333\n","buy     -0.333333\n","awesome  1.000000\n","better   0.142857"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"pYWjESWJtw9y"},"source":["# Test the Model"]},{"cell_type":"code","metadata":{"id":"-0JS4mxS6UWl"},"source":["def predict(data, sentiment_terms_frequency):\n","    generate_aspect_words(data)\n","    unk_words = set() \n","    for i in range(len(data)):\n","        for j in range(len(data[i].generated_aspect_words)):\n","            sentiment_terms = data[i].generated_aspect_words[j].sentiment_terms\n","            unk = 0\n","            polarity = 0\n","            for term in sentiment_terms:\n","                if term not in sentiment_terms_frequency.keys():\n","                    unk += 1\n","                    unk_words.add(term)\n","                else:\n","                    polarity += sentiment_terms_frequency[term]\n","            if (len(sentiment_terms) - unk != 0):\n","                data[i].generated_aspect_words[j].polarity = polarity/(len(sentiment_terms)-unk)\n","            else:\n","                data[i].generated_aspect_words[j].polarity = 0\n","    return len(unk_words)\n","\n","def find_polarity(value, threshold = 0.1):\n","    if (value < -1*threshold):\n","        return \"negative\"\n","    elif (value > threshold):\n","        return \"positive\"\n","    else:\n","        return \"neutral\"\n","\n","def classify(data, sentiment_terms_frequency):\n","    no_of_unk_words = predict(data, sentiment_terms_frequency)\n","    for i in range(len(data)):\n","        for j in range(len(data[i].generated_aspect_words)):\n","            data[i].generated_aspect_words[j].polarity = find_polarity(data[i].generated_aspect_words[j].polarity)\n","    return no_of_unk_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Ow5AwcMCNmN","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1638779091688,"user_tz":-330,"elapsed":4565,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"effca6a9-0cc5-4a66-b5e7-a6a196fcbdfa"},"source":["laptop_test_1_data = xml_to_sentences(root_dir + test_1_folder + laptop_test_1_file, data_type = \"Test\")\n","laptop_test_1_data_df = data_list_to_df(laptop_test_1_data, aspect_terms = \"none\", aspect_polarity = False)\n","laptop_test_1_data_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Boot time is super fast, around anywhere from ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>tech support would not fix the problem unless ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>but in resume this computer rocks!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Set up was easy.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Did not enjoy the new Windows 8 and touchscree...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  ID                                           Sentence\n","0  1  Boot time is super fast, around anywhere from ...\n","1  2  tech support would not fix the problem unless ...\n","2  3                 but in resume this computer rocks!\n","3  4                                   Set up was easy.\n","4  5  Did not enjoy the new Windows 8 and touchscree..."]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"ZzwFWk-WFZdY","executionInfo":{"status":"ok","timestamp":1638779233610,"user_tz":-330,"elapsed":107873,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"70490fa3-9252-40a4-c859-f82c9a5e5c72"},"source":["laptop_test_1_predictions_unk_count = classify(laptop_test_1_data, laptop_sentiment_terms_frequency_train)\n","print(\"Number of Unknown Sentiment Terms:\", laptop_test_1_predictions_unk_count)\n","laptop_test_1_data_df = data_list_to_df(laptop_test_1_data, aspect_terms = \"generated\", aspect_polarity = True)\n","laptop_test_1_data_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Unknown Sentiment Terms: 279\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sentence</th>\n","      <th>Generated Aspect Terms</th>\n","      <th>Generated Sentiment Terms</th>\n","      <th>Generated Polarities</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Boot time is super fast, around anywhere from ...</td>\n","      <td>[Boot time]</td>\n","      <td>[[fast]]</td>\n","      <td>[positive]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>tech support would not fix the problem unless ...</td>\n","      <td>[tech support, problem, plan]</td>\n","      <td>[[fix], [fix], [bought]]</td>\n","      <td>[negative, negative, negative]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>but in resume this computer rocks!</td>\n","      <td>[computer]</td>\n","      <td>[[rocks]]</td>\n","      <td>[neutral]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Set up was easy.</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Did not enjoy the new Windows 8 and touchscree...</td>\n","      <td>[functions, Windows]</td>\n","      <td>[[new, enjoy], [enjoy]]</td>\n","      <td>[neutral, neutral]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  ID  ...            Generated Polarities\n","0  1  ...                      [positive]\n","1  2  ...  [negative, negative, negative]\n","2  3  ...                       [neutral]\n","3  4  ...                              []\n","4  5  ...              [neutral, neutral]\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"4UhcH3uWuLym"},"source":["# Performance of Sentiment Classification"]},{"cell_type":"code","metadata":{"id":"Ys0Azbo3O_e-"},"source":["# Split data and do this\n","def classification_performance(data):\n","    matches = 0\n","    non_matches = 0\n","    for i in range(len(data)):\n","        for actual_aspect_word in data[i].actual_aspect_words:\n","            actual_aspect_term = actual_aspect_word.aspect_term\n","            actual_polarity = actual_aspect_word.polarity\n","            for generated_aspect_word in data[i].generated_aspect_words:\n","                if (match_aspect_terms(actual_aspect_term, generated_aspect_word.aspect_term) > 0):\n","                    if (actual_polarity == generated_aspect_word.polarity):\n","                        matches+=1\n","                    else:\n","                        non_matches+=1\n","                    break\n","\n","    return {\"Sentiment Matches\":matches, \"Sentiment Non-Matches\":non_matches, \"Sentiment Classification Precision\":matches/(matches+non_matches)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSweiDRCqo-s","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1638779512502,"user_tz":-330,"elapsed":24763,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"dcec674d-aba7-49be-8c13-9d9c232f1cb0"},"source":["laptop_valid_data_predictions_unk_count = classify(laptop_valid_data, laptop_sentiment_terms_frequency_train)\n","print(\"Unknown Sentiment Terms:\", laptop_valid_data_predictions_unk_count)\n","laptop_valid_data_df = data_list_to_df(laptop_valid_data, aspect_terms = \"none\", aspect_polarity = False)\n","laptop_valid_data_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unknown Sentiment Terms: 95\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>433</td>\n","      <td>WHEN TYPING, LETTERS AND SPACES ARE FREQUENTLY...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>59</td>\n","      <td>I love the glass touchpad.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1021</td>\n","      <td>Later it held zero charge and its replacemen...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>678</td>\n","      <td>This computer is exceptionally thin for it's s...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1319</td>\n","      <td>It has just enough RAM to run smoothly and eno...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     ID                                           Sentence\n","0   433  WHEN TYPING, LETTERS AND SPACES ARE FREQUENTLY...\n","1    59                         I love the glass touchpad.\n","2  1021    Later it held zero charge and its replacemen...\n","3   678  This computer is exceptionally thin for it's s...\n","4  1319  It has just enough RAM to run smoothly and eno..."]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"ttq7OdZduUp7","executionInfo":{"status":"ok","timestamp":1638779512506,"user_tz":-330,"elapsed":40,"user":{"displayName":"Shetty Karan Kavita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03089817666966117640"}},"outputId":"8c3a72e7-30c7-43af-808a-f6d43ec714a3"},"source":["laptop_valid_data_ratios = aspect_term_performance(laptop_valid_data)\n","laptop_valid_data_ratios.update(classification_performance(laptop_valid_data))\n","pd.DataFrame.from_dict(laptop_valid_data_ratios, orient = 'index',columns = [\"Parameters\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Parameters</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Aspect Term Extraction Precision</th>\n","      <td>0.467085</td>\n","    </tr>\n","    <tr>\n","      <th>Aspect Term Extraction Recall</th>\n","      <td>0.607576</td>\n","    </tr>\n","    <tr>\n","      <th>Aspect Term Extraction True Positives</th>\n","      <td>155.539348</td>\n","    </tr>\n","    <tr>\n","      <th>Actual Aspect Terms</th>\n","      <td>256.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Generated Aspect Terms</th>\n","      <td>333.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Sentiment Matches</th>\n","      <td>87.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Sentiment Non-Matches</th>\n","      <td>82.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Sentiment Classification Precision</th>\n","      <td>0.514793</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       Parameters\n","Aspect Term Extraction Precision         0.467085\n","Aspect Term Extraction Recall            0.607576\n","Aspect Term Extraction True Positives  155.539348\n","Actual Aspect Terms                    256.000000\n","Generated Aspect Terms                 333.000000\n","Sentiment Matches                       87.000000\n","Sentiment Non-Matches                   82.000000\n","Sentiment Classification Precision       0.514793"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"ZoT75a8kI7OR"},"source":["def write_to_csv(data, path):\n","    data_df = data_list_to_df(data, aspect_terms = \"both\", aspect_polarity = True)\n","    data_df.to_csv(path_or_buf = path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yO3IO42iNgmC"},"source":["# write_to_csv(laptop_train_data, root_dir + train_folder + \"generated_laptop_train_data.csv\")\n","# write_to_csv(laptop_test_1_data, root_dir + train_folder + \"generated_laptop_test_1_data.csv\")\n","# write_to_csv(laptop_valid_data, root_dir + train_folder + \"generated_laptop_valid_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oFBWz70ewcwU"},"source":["# Demo\n"]},{"cell_type":"code","metadata":{"id":"1FnNM0E8whFL"},"source":["def test(string):\n","  test_data = []\n","  test_data.append(string)\n","  test_predictions_unk_count = predict(test_data, mixed_sentiment_terms_frequency_train)\n","  print(test_data.head())\n","  classify(test_predictions, mixed_sentiment_terms_frequency_train)\n","  print(\"Unknown Sentiment Words:\", test_predictions_unk_count)\n","  print(\"\")\n","  print(\"Analysis:\")\n","  print(test_predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30NZ1wCl_cdi"},"source":["test(\"It is a good laptop but bad mouse\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BoOTyO3KW8ZM"},"source":["test(\"It is super fast and has outstanding graphics.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OaSOI7MZW8F7"},"source":["test(\"Laptop good, mouse bad\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JHhV1SFXvoj"},"source":["## Shortcomings and improvements\n","\n","* Compound aspect terms\n","* Compount sentiment words\n","* More rules, curent ones don't handle many cases \n","* Improve accuracy\n","* Precision of genration of aspect terms is an approximation\n","* Can try Ngram model - for 3 cases with aspect-aspect\n","\n"]}]}